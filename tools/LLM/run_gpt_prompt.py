import os
import re
from tools.LLM.ollama_agent import *
import sys

# ä¿®æ”¹å½“å‰å·¥ä½œç›®å½•
os.chdir('../')


api_url = "http://127.0.0.1:11434/api"
ollama_agent = OllamaAgent("qwen2.5:14b", api_url, "agent_chat")

    # agent1_name = 'ä¸¹å°¼'
    # agent2_name = 'è‹å…‹'
    #
    # positiuon = 'å¨æˆ¿'
    # agent1_memory = 'ä¸¹å°¼è¿™å‡ å¤©åœ¨å­¦æ ¡ï¼Œä½†æ˜¯å¬è¯´å¦ˆå¦ˆå’Œçˆ¶äº²è¦å»æ—…æ¸¸ï¼Œä»Šå¤©ä¸¹å°¼æ‰æ”¾å‡å›å®¶è§åˆ°çˆ¶æ¯'
    # agent2_memory = 'è‹å…‹è€å©†å¤©å¤©åŠ ç­å¿ƒæƒ…å¾ˆä¸å¥½ï¼Œæ‰€ä»¥è‹å…‹å’Œè€å©†å•†é‡å»ä¸Šæµ·æ—…æ¸¸ä¸‰å¤©ï¼Œè¿™å‡ å¤©è‹å…‹ä¹Ÿå¤©å¤©åŠ ç­ï¼Œè¿˜æ²¡å’Œå­©å­è¯´è¿™äº‹æƒ…ï¼Œä»Šå¤©å›å®¶ä¹Ÿä¸€ç›´åœ¨å¨æˆ¿åšé¥­ï¼Œåˆšçœ‹åˆ°ä¸¹å°¼èµ°è¿›å¨æˆ¿'
    #
    # x = double_agents_chat(positiuon,agent1_name,agent2_name,'',agent1_memory,agent2_memory)
    #
    # print(x)
    # print(type(x))

can_go_place = ['åŒ»é™¢', 'å’–å•¡åº—', 'èœœé›ªå†°åŸ', 'å­¦æ ¡', 'å°èŠ³å®¶', 'ç«é”…åº—', 'ç»¿é“', 'å°æ˜å®¶', 'å°ç‹å®¶', 'è‚¯å¾·åŸº',
                    'ä¹¡æ‘åŸº', 'å¥èº«æˆ¿', 'ç”µå½±é™¢', 'å•†åœº', 'æµ·è¾¹']


# æ¯æ—¥è®¡åˆ’è¡¨
def run_gpt_prompt_generate_hourly_schedule(persona,now_time):
    def create_prompt_input(persona,
                            p_f_ds_hourly_org,
                            hour_str,
                            intermission2=None,
                            test_input=None):
        pass

    def __func_clean_up(gpt_response, prompt=""):

        cr = gpt_response
        return cr

    def __func_validate(gpt_response, prompt=""):
        try:
            gpt_response = gpt_response.replace("```","").split("json")[1][1:]
            gpt_response = json.loads(gpt_response.strip('\n'))['output']
            total_time = sum(item[1] for item in gpt_response)
            # print(total_time)
            if total_time > 1920:
                return False
            __func_clean_up(gpt_response, prompt="")
        except:
            return False
        return True



    generate_prompt = OllamaAgent.generate_prompt(
        [persona,now_time],
        r"./tools/LLM/prompt_template/ç”Ÿæˆæ—¥ç¨‹å®‰æ’æ—¶é—´è¡¨.txt")
    output = ollama_agent.ollama_safe_generate_response(generate_prompt, "", "ä½ ä¸éœ€è¦è°ƒæ•´ï¼Œåªéœ€è¦ç»™æˆ‘è¾“å‡ºä¸€ä¸ªæœ€ç»ˆçš„ç»“æœï¼Œæˆ‘éœ€è¦ä¸€ä¸ªæ ‡å‡†çš„æ•°ç»„æ ¼å¼", 3,
                                                        __func_validate, __func_clean_up)

    if "json" in output:
        output = output.replace("```", "").split("json")[1][1:]
        output = json.loads(output.strip('\n'))['output']
        return output

    else:
        # print(output)
        return output


# æ¯å¤©è‹é†’æ—¶é—´
def run_gpt_prompt_wake_up_hour(persona,now_time,hourly_schedule):
    def __func_clean_up(gpt_response, prompt=""):
        cr = gpt_response
        return cr

    def __func_validate(gpt_response, prompt=""):
        try:
            if "output" in gpt_response:
                pattern = r'"output"\s*:\s*"([^"]+)"'
                match = re.search(pattern, gpt_response)
                output_value = match.group(1)
                __func_clean_up(output_value, prompt="")
            else:
                return False
        except:
            return False
        return True
    generate_prompt = OllamaAgent.generate_prompt(
        [persona,now_time,hourly_schedule],
        r"./tools/LLM/prompt_template/èµ·åºŠæ—¶é—´.txt")
    output = ollama_agent.ollama_safe_generate_response(generate_prompt, "",
                                                        "åªéœ€è¦ç»™æˆ‘è¾“å‡ºä¸€ä¸ªæœ€ç»ˆçš„ç»“æœä¸éœ€è¦ç»™æˆ‘å…¶ä»–ä»»ä½•ä¿¡æ¯ï¼Œæˆ‘éœ€è¦ä¸€ä¸ªæ ‡å‡†çš„æ—¥æœŸæ ¼å¼ï¼Œæ¯”å¦‚ï¼š07-01ï¼ˆè¡¨ç¤ºæ—©ä¸Šä¸ƒç‚¹é›¶ä¸€åˆ†èµ·åºŠï¼‰",
                                                        3,
                                                        __func_validate, __func_clean_up)
    pattern = r'"output"\s*:\s*"([^"]+)"'
    match = re.search(pattern, output)
    output = match.group(1)
    return output

# è¡ŒåŠ¨è½¬è¡¨æƒ…
def run_gpt_prompt_pronunciatio(Action_dec):
    def __chat_func_clean_up(gpt_response):  ############
        cr = gpt_response.strip()
        if len(cr) > 3:
            cr = cr[:3]
        return cr
    def __chat_func_validate(gpt_response):  ############
        try:
            gpt_response = json.loads(gpt_response)["output"]
            __chat_func_clean_up(gpt_response)
        except:
            return False
        return True
    example_output = "ğŸ›ğŸ§–â€â™€ï¸"  ########
    special_instruction = "è¾“å‡ºåªåŒ…å«è¡¨æƒ…ç¬¦å·"  ########
    generate_prompt = OllamaAgent.generate_prompt(
        [Action_dec],
        r"./tools/LLM/prompt_template/è¡Œä¸ºè½¬ä¸ºå›¾æ ‡æ˜¾ç¤º.txt")
    output = ollama_agent.ollama_safe_generate_response(generate_prompt, example_output, special_instruction, 7,__chat_func_validate,__chat_func_clean_up,'{"output":"ğŸ§˜ï¸"}')
    return json.loads(output)['output']


# ä¸¤ä¸ªæ™ºèƒ½ä½“é—´çš„å¯¹è¯-test
def double_agents_chat(maze,agent1_name,agent2_name,curr_context,init_summ_idea,target_summ_idea):
    def __chat_func_clean_up(gpt_response):  ############
        return gpt_response

    def __chat_func_validate(gpt_response):  ############
        try:
            __chat_func_clean_up(gpt_response)
        except:
            return False
        return True

    generate_prompt = OllamaAgent.generate_prompt(
        [maze,agent1_name, agent2_name, curr_context, init_summ_idea, target_summ_idea], r"./tools/LLM/prompt_template/èŠå¤©.txt")

    example_output = '[["ä¸¹å°¼", "ä½ å¥½"], ["è‹å…‹", "ä½ ä¹Ÿæ˜¯"] ... ]'
    special_instruction = 'è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ªåˆ—è¡¨ç±»å‹ï¼Œå…¶ä¸­å†…éƒ¨åˆ—è¡¨çš„å½¢å¼ä¸º[â€œ<åå­—>â€ï¼Œâ€œ<è¯è¯­>â€]ã€‚'

    x = ollama_agent.ollama_safe_generate_response(generate_prompt, example_output, special_instruction, 3,__chat_func_validate,__chat_func_clean_up)

    return json.loads(x)['output']


# åˆ¤æ–­åšè¿™ä»¶äº‹æƒ…éœ€è¦å»å“ªä¸ªåœ°æ–¹
def go_map(agent_name, home , curr_place, can_go, curr_task):
    def __chat_func_clean_up(gpt_response):  ############
        return gpt_response

    def __chat_func_validate(gpt_response):  ############
        try:
            if "output" in gpt_response:
                pattern = r'"output"\s*:\s*"([^"]+)"'
                match = re.search(pattern, gpt_response)
                output_value = match.group(1)
                __chat_func_clean_up(output_value)
            else:
                return False
        except:
            return False
        return True

    example_output = 'æµ·è¾¹'
    special_instruction = ''

    generate_prompt = OllamaAgent.generate_prompt(
        [agent_name,home , curr_place, can_go, curr_task],
        r"./tools/LLM/prompt_template/è¡ŒåŠ¨éœ€è¦å»çš„åœ°æ–¹.txt")

    output = ollama_agent.ollama_safe_generate_response(generate_prompt, example_output, special_instruction, 3,__chat_func_validate,__chat_func_clean_up)
    pattern = r'"output"\s*:\s*"([^"]+)"'
    match = re.search(pattern, output)
    output = match.group(1)
    return output

# æ€»ç»“ä»Šå¤©çš„ä¸€åˆ‡å†™å…¥è®°å¿†æ–‡ä»¶
def tess():
    pass

if __name__ == '__main__':
    api_url = "http://127.0.0.1:11434/api"
    ollama_agent = OllamaAgent("qwen2.5:14b", api_url, "agent_chat")

    # agent1_name = 'ä¸¹å°¼'
    # agent2_name = 'è‹å…‹'
    #
    # positiuon = 'å¨æˆ¿'
    # agent1_memory = 'ä¸¹å°¼è¿™å‡ å¤©åœ¨å­¦æ ¡ï¼Œä½†æ˜¯å¬è¯´å¦ˆå¦ˆå’Œçˆ¶äº²è¦å»æ—…æ¸¸ï¼Œä»Šå¤©ä¸¹å°¼æ‰æ”¾å‡å›å®¶è§åˆ°çˆ¶æ¯'
    # agent2_memory = 'è‹å…‹è€å©†å¤©å¤©åŠ ç­å¿ƒæƒ…å¾ˆä¸å¥½ï¼Œæ‰€ä»¥è‹å…‹å’Œè€å©†å•†é‡å»ä¸Šæµ·æ—…æ¸¸ä¸‰å¤©ï¼Œè¿™å‡ å¤©è‹å…‹ä¹Ÿå¤©å¤©åŠ ç­ï¼Œè¿˜æ²¡å’Œå­©å­è¯´è¿™äº‹æƒ…ï¼Œä»Šå¤©å›å®¶ä¹Ÿä¸€ç›´åœ¨å¨æˆ¿åšé¥­ï¼Œåˆšçœ‹åˆ°ä¸¹å°¼èµ°è¿›å¨æˆ¿'
    #
    # x = double_agents_chat(positiuon,agent1_name,agent2_name,'',agent1_memory,agent2_memory)
    #
    # print(x)
    # print(type(x))

    can_go_place = ['åŒ»é™¢', 'å’–å•¡åº—', 'èœœé›ªå†°åŸ', 'å­¦æ ¡', 'å°èŠ³å®¶', 'ç«é”…åº—', 'ç»¿é“', 'å°æ˜å®¶', 'å°ç‹å®¶', 'è‚¯å¾·åŸº',
                    'ä¹¡æ‘åŸº', 'å¥èº«æˆ¿', 'ç”µå½±é™¢', 'å•†åœº', 'æµ·è¾¹']
    x = go_map('å°æ˜','å°æ˜å®¶','å­¦æ ¡',str(can_go_place),'åƒåˆé¥­')
    print(x)
    print(type(x))
